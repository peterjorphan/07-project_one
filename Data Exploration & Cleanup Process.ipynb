{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Cleanup Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We primarily used two sources of data:\n",
    "    - Taxi Data: Pubclicaly available csv file from the City of Chicago\n",
    "    - Historical Weather Data: Dark Sky API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Data\n",
    "We downloaded the Taxi_Trips.csv file from the City of Chicago Data Portal (https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data) and saved it in our Resources folder. This file contains over 101M rows with information about taxi rides over a period of several years. Such a large file could not be handled by excel, and it would also be larger than the file size allowed on github where we keep our remote repository. In addition, one of our concerns was that iterating through all these lines and making API calls with the information on each line would take a considerable time. For this reason we decided to select a random sample of 50,000 rides from this file work from that (Resources/Taxi_Trips_Random_Sample.csv). This was accomplished by use of the following command in Git Bash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf -n 50000 Taxi_Trips.csv > Taxi_Trips_Random_Sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file generated this way was randomly sorted and was lacking headers. The random order is desirable for our analysis, however we decided to manually add the headers from the original file to make our analysis easier, and saved this as a separate file (Resources/DataWithHeaders.csv). The original large file was ignored in .gitignore so it doesn't get pushed to the remote repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "We extracted historic weather data by making calls using the Dark Sky API for different coordinates/timestamps in the taxi ride file. \n",
    "\n",
    "This API has a restriction to 1000 calls per user per day. Given the large data set we have, this caused a problem for collecting the points we need. We built our code in \"api_calls.ipynb\" so that every time we run it, it saves the maximum number of responses and appends the results inside a csv file (Output/weather.csv). The Trip ID code was included in that file which would allow us to later merge this with our other file. We built the code so that it recognizes what was the last line pulled, and continues from there. \n",
    "\n",
    "Using this system, we were able to run this multiple times with different User IDs/keys and repeat this in different days to collect a larger number of observations. This gave us a working file of 8842 lines as of 3/7/19.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged File\n",
    "We used pandas to create DataFrames from the above two csv files. The two DataFrames were joined on the Trip ID field with a regular join to allow for further analysis. This kept only the lines for which we had weather information. \n",
    "\n",
    "We noted that the time stamp was given in a Date and Time format separated by the letter 'T'. For the purposes of this analysis we split this into a \"Date\" and \"Time\" field and saved in separate columns. In addition we isolated the month as a separate column to use as a group for our analysis.\n",
    "\n",
    "Finally, during this phase, we created a heatmap from the merged file showing the number of rides in different neighbohoods of Chicago. This was done so we get more familiar with our dataset and see what geographical areas were covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Data\n",
    "In planning for our analysis we noted that counting the number of rides for weather groups or other weather related fields would give us a skewed estimate of the rides in correlation with the weather, because differnet types of weather do not occur with the same frequency during the time period of our dataset. Therefore, rides occuring during the most common weather would be arteficially inflated, while that larger number has nothing to do with the weather itself (just to the fact that there are more days with this weather in our dataset).\n",
    "\n",
    "To resolve this issue we decided to measure the rides by counting them per day within the group evaluated (e.g. Weather Groups), and then taking the average inside that group. This would give us results that are much more comparable to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Considerations\n",
    "Our original planning involved uber data instead of Taxi data, as well as fuel prices. After initial exploration it was determined that the Uber data would not be easy to obtain (an API is not available), and therefore switched to the Taxi ride data used here. As such, our fuel price data were not relevant anymore because taxi rates are fixed and don't fluctuate as is the case with Uber. Nevertheless, a file with fuel prices was kept even though it wasn't used in this analysis (Resources/GASREGW.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
